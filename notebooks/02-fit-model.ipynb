{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting\n",
    "Purpose of this script is to use the data formatted in 01 to fit a grouped lasso model to the data, and make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'dplyr' was built under R version 3.3.2\"Warning message:\n",
      "\"package 'grplasso' was built under R version 3.3.2\""
     ]
    }
   ],
   "source": [
    "options(stringsAsFactors = FALSE)\n",
    "library(dplyr, quietly = TRUE, warn.conflicts = FALSE)\n",
    "library(grplasso, quietly = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>\"cat_mm_test\"</li>\n",
       "\t<li>\"grp_ind_1\"</li>\n",
       "\t<li>\"grp_ind_2\"</li>\n",
       "\t<li>\"num_vars_test\"</li>\n",
       "\t<li>\"test\"</li>\n",
       "\t<li>\"train\"</li>\n",
       "\t<li>\"x1\"</li>\n",
       "\t<li>\"x2\"</li>\n",
       "\t<li>\"y1\"</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item \"cat\\_mm\\_test\"\n",
       "\\item \"grp\\_ind\\_1\"\n",
       "\\item \"grp\\_ind\\_2\"\n",
       "\\item \"num\\_vars\\_test\"\n",
       "\\item \"test\"\n",
       "\\item \"train\"\n",
       "\\item \"x1\"\n",
       "\\item \"x2\"\n",
       "\\item \"y1\"\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. \"cat_mm_test\"\n",
       "2. \"grp_ind_1\"\n",
       "3. \"grp_ind_2\"\n",
       "4. \"num_vars_test\"\n",
       "5. \"test\"\n",
       "6. \"train\"\n",
       "7. \"x1\"\n",
       "8. \"x2\"\n",
       "9. \"y1\"\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"cat_mm_test\"   \"grp_ind_1\"     \"grp_ind_2\"     \"num_vars_test\"\n",
       "[5] \"test\"          \"train\"         \"x1\"            \"x2\"           \n",
       "[9] \"y1\"           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read in data from 01\n",
    "load('../intermed-rdatas/m1_objects.RData')\n",
    "load('../intermed-rdatas/m2_objects.RData')\n",
    "load('../intermed-rdatas/test_objects.RData')\n",
    "\n",
    "train <- read.csv('../data/train.csv')\n",
    "test <- read.csv('../data/test.csv')\n",
    "\n",
    "# What have we just imported?\n",
    "ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Test Data\n",
    "Not all test data will be used to fit model 1.  Many test observations have levels of variables X0, X2, and X5 that are not in the training data.  We will also need to fill in columns with zeros that are missing from the test data but included in the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "for_m1\n",
       "FALSE  TRUE \n",
       "   24  4185 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for_m1 <- (test$X0 %in% unique(train$X0)) &\n",
    "          (test$X2 %in% unique(train$X2)) &\n",
    "          (test$X5 %in% unique(train$X5))\n",
    "\n",
    "table(for_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep rows for use in m1\n",
    "x1_test <- cbind(1, cat_mm_test, num_vars_test)[for_m1,]\n",
    "# Keep cols that are in x1\n",
    "x1_test <- x1_test[colnames(x1_test) %in% colnames(x1)]\n",
    "# Fill remainders with 0\n",
    "for (cn in setdiff(colnames(x1), colnames(x1_test))) x1_test[cn] <- 0\n",
    "# Reorder to match x1\n",
    "x1_test <- x1_test[colnames(x1)]\n",
    "    \n",
    "# Repeat for model 2\n",
    "x2_test <- cbind(1, cat_mm_test, num_vars_test)[!for_m1,]\n",
    "x2_test <- x2_test[colnames(x2_test) %in% colnames(x2)]\n",
    "for (cn in setdiff(colnames(x2), colnames(x2_test))) x2_test[cn] <- 0\n",
    "x2_test <- x2_test[colnames(x2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Models\n",
    "Finally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in grplasso.default(x1, y1, index = grp_ind_1, lambda = lambda, :\n",
      "\"Penalization not adjusted to non-penalized predictors.\"Warning message in grplasso.default(x1, y1, index = grp_ind_1, lambda = lambda, :\n",
      "\"Maximal number of iterations reached for lambda[1]\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 10  nr.var: 307 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in grplasso.default(x2, y1, index = grp_ind_2, lambda = lambda, :\n",
      "\"Penalization not adjusted to non-penalized predictors.\"Warning message in grplasso.default(x2, y1, index = grp_ind_2, lambda = lambda, :\n",
      "\"Maximal number of iterations reached for lambda[1]\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 10  nr.var: 231 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   user  system elapsed \n",
       " 353.84    0.78  364.10 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lambda <- 10\n",
    "system.time({\n",
    "    m1 <- grplasso(x1, y1, index = grp_ind_1, lambda = lambda, model = LinReg())\n",
    "    m2 <- grplasso(x2, y1, index = grp_ind_2, lambda = lambda, model = LinReg())\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yhat_test1 <- exp(predict(m1, x1_test))\n",
    "yhat_test2 <- exp(predict(m2, x2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_preds <- rep(0, nrow(test))\n",
    "final_preds[for_m1] <- yhat_test1\n",
    "final_preds[!for_m1] <- yhat_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_out <- data.frame(ID = test$ID, y = final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write.csv(df_out, '../output/lambda10.csv', row.names = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "For various values of $\\lambda$, my testing score (measured in $R^2$) tends to float between 52% and 54%, with my high score being almost 55%.  The high score on the leaderboard is about 57.5%.  In a real life scenario, these would be comparable scores.  On Kaggle, however, this does not even land me in the top 67% of user submissions!  \n",
    "\n",
    "Again, in practice, this model took me a combined 10 hours to write (the fitting process takes about 5 min).  Judging from user-submitted kernels of participants using neural net-style models, theirs took a lot longer to write, and likely hours to fit.  My models were run locally, as I did not opt to use AWS or any other similar platform.  Given my shoestring-budget model, I am very proud of these results.\n",
    "\n",
    "I'm very curious to see if any other users scored above 55% using a model that was neither a neural net nor a tree method.  I suspect not 8).\n",
    "\n",
    "Someone with more time on his or her hands could likely improve my model by:\n",
    "* Actually using some sort of cross-validation to select the optimal $\\lambda$.\n",
    "* Using different $\\lambda$s for `m1` and `m2`.\n",
    "* The residual plot is actually terrible.  I believe there is some sort of clustering that can be incorporated to renormalize the residuals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
